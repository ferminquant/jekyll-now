---
layout: post
title: Playing with Many Classification Algorithms on the Titanic Data Set
---

In this post I make a comparison of different classification algorithms, using the training data from [Kaggle](https://www.kaggle.com/c/titanic/data). The data is small, and you can't get more, so the full difference between algorithms probably won't be noticed. I put up the whole code on my [Github](https://github.com/ferminquant/TitanicSurvival).

<!-- MarkdownTOC autolink="true" bracket="round" depth="0" style="unordered" indent="  " autoanchor="false" -->

- [Linear Model](#linear-model)

<!-- /MarkdownTOC -->

# Linear Model

The first algorithm used will be a simple linear model, for which I used the lm function from R.

```r 
set.seed(1320)
data = read.csv("train.csv", header = TRUE)
data$Age = ifelse(is.na(data$Age), -1, data$Age)
formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked
```

As a first step I loaded the data, replaced all NA ages with -1, and decided on ignoring the following columns:

- PassengerId: it is a unique identifier for each passenger, I found no way for it to be helpful for predicting survival.
- Name: same as with PassengerId.
- Ticket: again, same as the two above.
- Cabin: might have been useful, but 77% of the data is empty, and some passengers have many cabins. If it had a more complete data set, and something in the Cabin name, like the preceding letter indicated something useful (like the position in the ship), I would have not ignored it.

The ones I did include are:

- Pclass: the passenger class, because first class might have been in a more "escapable" position, or would have been given priority.
- Sex: usually on such emergencies, the call is to save women and children first, data will tell us if it held true this time.
- Age: as with sex, younger women and children are given priority.
- SibSp: if they had siblings or a spouse onboard, as in someone to help them.
- Parch: parents or children, again someone to help you survive, or as a parent sacrifice yourself for your children, could be possibilities.
- Fare: how much they paid for their ticket, rich people usually get priority in our unfair society, although in emergencies it might not hold true, we have to test it.
- Embarked: where they embarked, it could hold more information about their background.

```r
for(j in 1:100){
}
```
I repeat the code 100 times and get an average, just as an extra measure to prevent overfitting.

```r
  data = data[sample(NROW(data)),]
```
At the start of each cycle, I shuffle the data randomly.
  
```r
  n_folds = 10
  folds = cut(seq(1,NROW(data)),breaks=n_folds,labels=FALSE)
  
  for(i in 1:n_folds){
  }
```
Then I start a 10 fold cross validation, again to avoid overfitting.

```r
    indexes = which(folds==i,arr.ind=TRUE)
    test_data = data[indexes, ]
    train_data = data[-indexes, ]
    
    model = lm(formula, train_data)
```
Separate train data from test data. For each cross validation iteration 9 parts to train data and 1 part to test data.
Then, train the linear model with the lm function.

```r    
    # for cases when test data has values not in train data
    source("missingLevelsToNA.R")
    test_data = missingLevelsToNA(model,test_data)
```
This code is to prevent the next line from failing. I believe this should be done automatically in the predict function, but the coder who made the R function thought differently. What happens is that the Embarked column has only two values for "" or empty string. If those two values happen to stay in the test data, then the model does not know them, and can't evaluate them, and fails. In my opinion, it should just throw a warning. I got the code to replace the values not in the training data with NA in the test data from [here](http://stackoverflow.com/questions/4285214/predict-lm-with-an-unknown-factor-level-in-test-data).

```r    
    test_data$prediction = predict(model, test_data)
```
Code to predict data from the test data using the resulting model. The results are created as a new column in the test data called prediction, which is a real number between 0 and 1, indicating the probability of surviving or not.

```r    
    threshold = 0.5
    test_data$result = ifelse(test_data$Survived == 0 & test_data$prediction <  threshold, 'TN', 
                              ifelse(test_data$Survived == 0 & test_data$prediction >= threshold, 'FP', 
                                     ifelse(test_data$Survived == 1 & test_data$prediction >= threshold, 'TP', 'FN')))
    
    tmp = table(test_data$result)
    
    if(is.na(tmp['FN'])){FN = 0} else {FN = tmp['FN']}
    if(is.na(tmp['FP'])){FP = 0} else {FP = tmp['FP']}
    if(is.na(tmp['TN'])){TN = 0} else {TN = tmp['TN']}
    if(is.na(tmp['TP'])){TP = 0} else {TP = tmp['TP']}
```
I decided on the simplest threshold of 0.5, meaning if the prediction is at least 0.5 then it is predicting it survived. Then I evaluate the confusion matrix, taking each value into a variable for future calculations:

- TN: True Negative, didn't survive and predicted to not survive.
- FP: False Positive, didn't survive and predicted to survive.
- TP: True Positive, survived and predicted to survive.
- FN: False Negative, survived and predicted to not survive.
    
```r    
    accuracy[i] = as.numeric((TN+TP)/sum(tmp))
    error[i] = as.numeric((FN+FP)/sum(tmp))
    
    #of all predicted positives, how many were really positive
    precision[i] = as.numeric(TP/(TP+FP))
    
    #of all positives, how many were predicted as positive
    #also know as sensitivity or true positive rate
    recall[i] = as.numeric(TP/(TP+FN))
    
    #of all negatives, how many were predicted as negative
    specificity[i] = as.numeric(TN/(TN+FP))
    
    fscore[i] = 2*((precision[i]*recall[i])/(precision[i]+recall[i]))
    
    #of all negatives, how many were predicted as positive
    false_positive_rate[i] = as.numeric(FP/(TN+FP))
```
Now, using the metrics from before, I evaluate other metrics to get a better idea of the model performance:

- Accuracy: percentage of correct predictions from the test data.
- Error: percentage of incorrect predictions from the test data.
- Precision: of those predicted to survive, how many really survived.
- Recall: of those who survived, how many were predicted to survive.
- Specificity: of those who did not survived, how many were predicted to not survive.
- F1 Score: it gives you an idea of the accuracy. For more information read [this](https://en.wikipedia.org/wiki/F1_score).
- False Positive Rate: of those who did not survive, how many were predicted to survive.

After running the code in my "titanic_lm.R" file, these were my results:

| Model        | **Accuracy** | Error  | Precision | Recall | Specificity | F1 Score | FP Rate |
|:------------:|:------------:|:------:|:---------:|:------:|:-----------:|:--------:|:-------:|
| Linear Model | **78.16%**   | 21.84% | 73.27%    | 67.95% | 84.56%      | 70.20%   | 15.44%  |

For comparison between models, I will concentrate on the simple measure of accuracy. The trade-off between precision and recall, and the other metrics are more specific to the business problem you solve with these classification algorithms, which do not have much relevance here. They are good for reference, but no need to deeply analyze them here.

tuz