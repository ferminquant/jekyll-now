---
layout: post
title: Playing with Many Classification Algorithms on the Titanic Data Set
---

In this post I make a comparison of different classification algorithms, using the training data from [Kaggle](https://www.kaggle.com/c/titanic/data). The data is small, and you can't get more, so the full difference between algorithms probably won't be noticed. I put up the whole code on my [Github](https://github.com/ferminquant/TitanicSurvival).

<!-- MarkdownTOC autolink="true" bracket="round" depth="0" style="unordered" indent="  " autoanchor="false" -->

- [Linear Model](#linear-model)
- [Generalized Linear Model](#generalized-linear-model)
- [Neural Networks](#neural-networks)

<!-- /MarkdownTOC -->

# Linear Model

The first algorithm used will be a simple linear model, for which I used the lm function from R.

```r 
set.seed(1320)
data = read.csv("train.csv", header = TRUE)
data$Age = ifelse(is.na(data$Age), -1, data$Age)
formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked
```

As a first step I loaded the data, replaced all NA ages with -1, and decided on ignoring the following columns:

- PassengerId: it is a unique identifier for each passenger, I found no way for it to be helpful for predicting survival.
- Name: same as with PassengerId.
- Ticket: again, same as the two above.
- Cabin: might have been useful, but 77% of the data is empty, and some passengers have many cabins. If it had a more complete data set, and something in the Cabin name, like the preceding letter indicated something useful (like the position in the ship), I would have not ignored it.

The ones I did include are:

- Pclass: the passenger class, because first class might have been in a more "escapable" position, or would have been given priority.
- Sex: usually on such emergencies, the call is to save women and children first, data will tell us if it held true this time.
- Age: as with sex, younger women and children are given priority.
- SibSp: if they had siblings or a spouse onboard, as in someone to help them.
- Parch: parents or children, again someone to help you survive, or as a parent sacrifice yourself for your children, could be possibilities.
- Fare: how much they paid for their ticket, rich people usually get priority in our unfair society, although in emergencies it might not hold true, we have to test it.
- Embarked: where they embarked, it could hold more information about their background.

```r
for(j in 1:100){
}
```
I repeat the code 100 times and get an average, just as an extra measure to prevent overfitting.

```r
  data = data[sample(NROW(data)),]
```
At the start of each cycle, I shuffle the data randomly.
  
```r
  n_folds = 10
  folds = cut(seq(1,NROW(data)),breaks=n_folds,labels=FALSE)
  
  for(i in 1:n_folds){
  }
```
Then I start a 10 fold cross validation, again to avoid overfitting.

```r
    indexes = which(folds==i,arr.ind=TRUE)
    test_data = data[indexes, ]
    train_data = data[-indexes, ]
    
    model = lm(formula, train_data)
```
Separate train data from test data. For each cross validation iteration 9 parts to train data and 1 part to test data.
Then, train the linear model with the lm function.

```r    
    # for cases when test data has values not in train data
    source("missingLevelsToNA.R")
    test_data = missingLevelsToNA(model,test_data)
```
This code is to prevent the next line from failing. I believe this should be done automatically in the predict function, but the coder who made the R function thought differently. What happens is that the Embarked column has only two values for "" or empty string. If those two values happen to stay in the test data, then the model does not know them, and can't evaluate them, and fails. In my opinion, it should just throw a warning. I got the code to replace the values not in the training data with NA in the test data from [here](http://stackoverflow.com/questions/4285214/predict-lm-with-an-unknown-factor-level-in-test-data).

```r    
    test_data$prediction = predict(model, test_data)
```
Code to predict data from the test data using the resulting model. The results are created as a new column in the test data called prediction, which is a real number between 0 and 1, indicating the probability of surviving or not.

```r    
    threshold = 0.5
    test_data$result = ifelse(test_data$Survived == 0 & test_data$prediction <  threshold, 'TN', 
                              ifelse(test_data$Survived == 0 & test_data$prediction >= threshold, 'FP', 
                                     ifelse(test_data$Survived == 1 & test_data$prediction >= threshold, 'TP', 'FN')))
    
    tmp = table(test_data$result)
    
    if(is.na(tmp['FN'])){FN = 0} else {FN = tmp['FN']}
    if(is.na(tmp['FP'])){FP = 0} else {FP = tmp['FP']}
    if(is.na(tmp['TN'])){TN = 0} else {TN = tmp['TN']}
    if(is.na(tmp['TP'])){TP = 0} else {TP = tmp['TP']}
```
I decided on the simplest threshold of 0.5, meaning if the prediction is at least 0.5 then it is predicting it survived. Then I evaluate the confusion matrix, taking each value into a variable for future calculations:

- TN: True Negative, didn't survive and predicted to not survive.
- FP: False Positive, didn't survive and predicted to survive.
- TP: True Positive, survived and predicted to survive.
- FN: False Negative, survived and predicted to not survive.
    
```r    
    accuracy[i] = as.numeric((TN+TP)/sum(tmp))
    error[i] = as.numeric((FN+FP)/sum(tmp))
    precision[i] = as.numeric(TP/(TP+FP))
    recall[i] = as.numeric(TP/(TP+FN))
    specificity[i] = as.numeric(TN/(TN+FP))
    fscore[i] = 2*((precision[i]*recall[i])/(precision[i]+recall[i]))
    false_positive_rate[i] = as.numeric(FP/(TN+FP))
```
Now, using the metrics from before, I evaluate other metrics to get a better idea of the model performance:

- Accuracy: percentage of correct predictions from the test data.
- Error: percentage of incorrect predictions from the test data.
- Precision: of those predicted to survive, how many really survived.
- Recall: of those who survived, how many were predicted to survive.
- Specificity: of those who did not survived, how many were predicted to not survive.
- F1 Score: it gives you an idea of the accuracy. For more information read [this](https://en.wikipedia.org/wiki/F1_score).
- False Positive Rate: of those who did not survive, how many were predicted to survive.

After running the code in my "titanic_lm.R" file, these were my results:

| Model        | **Accuracy** | Error  | Precision | Recall | Specificity | F1 Score | FP Rate |
|:------------:|:------------:|:------:|:---------:|:------:|:-----------:|:--------:|:-------:|
| Linear Model | **78.16%**   | 21.84% | 73.27%    | 67.95% | 84.56%      | 70.20%   | 15.44%  |

For comparison between models, I will concentrate on the simple measure of accuracy. The trade-off between precision and recall, and the other metrics are more specific to the business problem you solve with these classification algorithms, which do not have much relevance here. They are good for reference, but no need to deeply analyze them here.

# Generalized Linear Model

The second algorithm is just a slight variation, the Generalized Linear Model, for which I used the glm function in R with a binomial distribution. All the code is the same as above, except where we train the model and predict the test values, which now is as follows:

```r
    model = glm(formula, family = binomial(link='logit'), train_data)
    
    test_data$prediction = predict(model, newdata=test_data, type='response')
```

After running the code in my "titanic_glm.R" file, these were my results:

| Model        | **Accuracy** | Error  | Precision | Recall | Specificity | F1 Score | FP Rate |
|:------------:|:------------:|:------:|:---------:|:------:|:-----------:|:--------:|:-------:|
| Linear Model | **78.16%**   | 21.84% | 73.27%    | 67.95% | 84.56%      | 70.20%   | 15.44%  |
| GLM          | **78.69%**   | 21.31% | 73.50%    | 69.66% | 84.36%      | 71.22%   | 15.64%  |

Overall, it didn't really improve much, I would even consider them to have the same performance, not that I expected much difference between lm and glm.

# Neural Networks

The third algorithm is neural networks. For this I used the neuralnet package in R:

```r
library(neuralnet)
```

From what I have read in a lot of places which I do not remember, it is recommended to have one hidden layer with at least the same amount of nodes as input nodes, and up to 4 times the input nodes. Since I am experimenting, I will ignore that advice, and try a lot of different configurations. Hopefully, something interesting comes out.

Since neural networks take much more time than lm or glm to train, I removed the 100 cycle loop, and will only do a regular 10 fold cross validation. For the neuralnet package, you need to ready the data to feed it into the neural network, as in transform the categorical variables into numerical values, which is done like this:

```r
data = model.matrix(~ Survived + Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data)
```

You also need to scale the values, so that all columns have values between 0 and 1. I achieved it like this:
```r
maxs = apply(data, 2, max)
mins = apply(data, 2, min)
data = as.data.frame(scale(data, center = mins, scale = maxs - mins))
```

Then I train the neural network. For a start I will try with one hidden layer with only one node, the lamest configuration I could think of.

```r
  model = neuralnet( 
    Survived ~ Pclass + Sexmale + Age + SibSp + Parch + Fare + EmbarkedC + EmbarkedQ + EmbarkedS, 
    data=train_data, hidden=c(1), threshold=0.05, stepmax=1e+07, lifesign='full',
    lifesign.step = 25000, rep=1, linear.output=FALSE
  )
```
A small explanation of the parameters I chose:

- hidden: this is a list indicating the layers and amount of nodes in each layer. For two layers with 3 nodes each you would input c(3,3). Since this is the first example, I just chose one layer with one node.
- threshold: this is the stopping criteria of the error function. The value 0.01 is the default value set by neuralnet. I chose 0.05 expecting accuracy to not suffer much, and avoid performance issues with my laptop CPU.
- stepmax: the maximum number of steps to train the network for it to converge. The default is 1e+05, which should be enough, I just felt like increasing it.
- lifesign: just a flag to show or not the progress of the training. Full chooses to show progress every x amount of steps, defined in the next parameter.
- lifesign.step: the amount of steps to wait to show an update to the training process, when lifesign "full" is chosen.
- rep: 1 is the default. It is the amount of times the network is trained. If greater than, at the end it chooses the best result. 
- linear.output: I chose FALSE since it is a classification problem, TRUE is for regression.

After training, there is no need to set missing values to NA like in lm and glm above, so that code was omitted. Now the prediction over the test data.

```r
  test_data$prediction = compute(model,test_data[,3:11])$net.result
```

The rest of the code for the confusion matrix and the other calculations is the same.
After running the code in my "titanic_nn.R" file, these were my results:

| Model        | **Accuracy** | Error  | Precision | Recall | Specificity | F1 Score | FP Rate | Time |
|:------------:|:------------:|:------:|:---------:|:------:|:-----------:|:--------:|:-------:|:----:|
| Linear Model | **78.16%**   | 21.84% | 73.27%    | 67.95% | 84.56%      | 70.20%   | 15.44%  | <1s  |
| GLM          | **78.69%**   | 21.31% | 73.50%    | 69.66% | 84.36%      | 71.22%   | 15.64%  | <1s  |
| NN(1)        | **79.91%**   | 20.09% | 83.45%    | 60.05% | 92.41%      | 69.29%   |  7.59%  | <1s  |

It got a little better than glm, however it seems to be a little more biased into correctly predicting those who did not survive, as the specificity is specially higher. Also, precision is higher, but recall got lower, so it seems it is more conservative on predicting survival, and the F1 Score is lower. 

In any case, this was the lamest neural network configuration I could think of, it has one node in one hidden layer, and it still outperformed lm and glm, seems interesting. I added a column to see how much time it took to train each cross validation iteration on my CPU, just for reference.

Now I will test a lot of configurations in the hidden layer, to see if they make a difference.

| Model        | **Accuracy** | Error  | Precision | Recall | Specificity | F1 Score | FP Rate | Time |
|:------------:|:------------:|:------:|:---------:|:------:|:-----------:|:--------:|:-------:|:----:|
| Linear Model | **78.16%**   | 21.84% | 73.27%    | 67.95% | 84.56%      | 70.20%   | 15.44%  | <1s  |
| GLM          | **78.69%**   | 21.31% | 73.50%    | 69.66% | 84.36%      | 71.22%   | 15.64%  | <1s  |
| NN(1)        | **79.91%**   | 20.09% | 83.45%    | 60.05% | 92.41%      | 69.29%   |  7.59%  | <1s  |
| NN(2)        | **79.34%**   | 20.66% | 82.74%    | 59.33% | 91.77%      | 68.46%   |  8.23%  | <1s  |
| NN(3)        | **80.46%**   | 19.54% | 84.18%    | 61.16% | 92.61%      | 70.23%   |  7.39%  | <1s  |
| NN(4)        | **79.90%**   | 20.10% | 79.57%    | 64.79% | 89.47%      | 71.02%   | 10.53%  | ~1s  |
| NN(5)        | **81.03%**   | 18.97% | 81.63%    | 65.17% | 90.97%      | 72.07%   |  9.03%  | ~2s  |
| NN(6)        | **80.46%**   | 19.54% | 78.56%    | 67.55% | 88.09%      | 72.38%   | 11.91%  | ~2s  |
| NN(7)        | **81.47%**   | 18.53% | 81.39%    | 67.40% | 90.04%      | 73.39%   |  9.96%  | ~2s  |
| NN(8)        | **80.46%**   | 19.54% | 78.24%    | 69.14% | 87.40%      | 72.82%   | 12.60%  | ~4s  |
| NN(9)        | **80.92%**   | 19.08% | 80.14%    | 67.21% | 89.33%      | 72.67%   | 10.67%  | ~5s  |
| NN(10)       | **80.13%**   | 19.87% | 77.81%    | 68.36% | 87.39%      | 72.06%   | 12.61%  | ~7s  |
| NN(20)       | **81.02%**   | 18.98% | 77.49%    | 71.49% | 86.97%      | 73.89%   | 13.03%  | ~12s |
| NN(30)       | **78.89%**   | 21.11% | 74.57%    | 68.83% | 85.12%      | 71.09%   | 14.88%  | ~20s |
| NN(40)       | **79.23%**   | 20.77% | 75.68%    | 68.04% | 86.28%      | 71.08%   | 13.72%  | ~30s |
| NN(50)       | **80.35%**   | 19.65% | 76.03%    | 71.56% | 86.25%      | 73.25%   | 13.75%  | ~40s |
| NN(60)       | **78.55%**   | 21.45% | 74.79%    | 67.08% | 85.74%      | 70.10%   | 14.26%  | ~2m  |
| NN(70)       | **78.56%**   | 21.44% | 73.58%    | 69.70% | 84.23%      | 70.98%   | 15.77%  | ~2m  |
| NN(80)       | **74.62%**   | 25.38% | 69.51%    | 75.44% | 74.86%      | 70.43%   | 25.14%  | ~1m  |
| NN(90)       | **79.57%**   | 20.43% | 75.55%    | 69.67% | 85.76%      | 71.87%   | 14.24%  | ~1m  |
| NN(100)      | **78.66%**   | 21.34% | 74.37%    | 68.13% | 85.37%      | 70.63%   | 14.63%  | ~1m  |

It seems from the results above that the recommendations gave a good results. Not much difference is seen, but the best results considering all metrics, seem to be with 20 neurons, 2x the amount of inputs, it has one of the highest accuracies in the 81% range, and has the highest F1 Score. 

Here is the image of the last model with 100 hidden neurons, just for fun's sake:

![100 hidden neurons]({{ site.baseurl }}/images/100-neuron-nn.png)

To finish this part of experimentation with neural networks, I am going to trim the other results before continuing:

| Model        | **Accuracy** | Error  | Precision | Recall | Specificity | F1 Score | FP Rate | Time |
|:------------:|:------------:|:------:|:---------:|:------:|:-----------:|:--------:|:-------:|:----:|
| Linear Model | **78.16%**   | 21.84% | 73.27%    | 67.95% | 84.56%      | 70.20%   | 15.44%  | <1s  |
| GLM          | **78.69%**   | 21.31% | 73.50%    | 69.66% | 84.36%      | 71.22%   | 15.64%  | <1s  |
| NN(20)       | **81.02%**   | 18.98% | 77.49%    | 71.49% | 86.97%      | 73.89%   | 13.03%  | ~12s |

<!--
Continue with multiple layers

then try other libraries
-->